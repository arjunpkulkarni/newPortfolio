---
title: "How I Cut p95 Latency by 30% Using Redis, Postgres, and Predictive Caching"
publishedAt: "2024-12-07"
summary: "Deep dive into optimizing vector search latency at Siramai through Redis caching, predictive query patterns, and pipeline batching — cutting p95 latency by 30%."
company: "Siramai"
role: "Founding Engineer"
technologies: ["Redis", "Postgres", "Python", "Vector Search", "Caching"]
category: "Systems Engineering"
---

At Siramai, our e-commerce OS generated **2,000+ daily vector search queries** across agents, ranking models, and retrieval pipelines. As the product scaled, we hit a wall: **p95 latency spiked above 450 ms** during peak hours.

This post breaks down how I redesigned our caching and data pipelines using **Redis, Postgres, batching, and adaptive caching strategies**, cutting **p95 latency by 30%** and stabilizing throughput.

## The Bottleneck: Non-Deterministic Query Paths

Our system had three latency contributors:

1. Vector search lookups
2. Ranking model calls
3. Product metadata joins in Postgres

Requests took wildly different paths depending on user history, availability of embeddings, and cache state — leading to unpredictable spikes.

## Architecture Before Optimization

```
Client → API Gateway → Vector Search → Ranker → Postgres → Response
```

Each query touched 2–3 separate datastores with no coordination.

## The Optimization Strategy

### 1. Predictive Caching for "Next Likely Queries"

We analyzed user paths and found:

* 63% of users performed similar searches within the first 5 minutes
* Category browsing had strong sequential patterns

So we built a **next-query prediction model**, caching:

* Neighboring embeddings
* Adjacent categories
* Pre-ranked product clusters

This alone shaved **~90 ms** off many hot paths.

### 2. Redis Write-Through Cache for Product Metadata

Postgres joins were expensive, so we created a single Redis hash structure:

```python
# Redis Hash Structure
product:{id}
  - title
  - description
  - brand
  - price
  - embeddings
```

Metadata that once required 2–3 Postgres hits became **one Redis lookup**.

### 3. Pipeline Batching

We introduced micro-batching for vector search:

* Instead of firing off queries individually
* We batch within an 8 ms window and share results across requests

This provided ~18% efficiency improvement during peak hours.

## Results

| Metric      | Before        | After      |
| ----------- | ------------- | ---------- |
| p95 Latency | **450 ms**    | **310 ms** |
| Throughput  | Baseline      | **1.4×**   |
| Consistency | High variance | Stable     |

## Key Lessons

* **Predictive caching is far more powerful than static caching** — understanding user behavior patterns can guide intelligent pre-fetching
* **Redis hashes outperform Postgres for repeated metadata fetches** — denormalization has its place in high-throughput systems
* **Small batching windows give enormous wins at scale** — even 8ms windows can aggregate multiple requests

## Future Work

* GPU-accelerated vector search (FAISS / cuVS)
* Asynchronous embeddings refresh
* Real-time ranking model distillation

