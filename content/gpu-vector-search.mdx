---
title: "Building a GPU-Accelerated Vector Search Engine (FAISS + Triton Kernels)"
publishedAt: "2024-11-15"
summary: "How I built a GPU-accelerated vector search engine using FAISS and custom Triton kernels to achieve 15x speedup over CPU-based retrieval."
company: "Siramai"
role: "Founding Engineer"
technologies: ["FAISS", "Triton", "CUDA", "Python", "GPU", "PyTorch"]
category: "AI Infrastructure"
---

At Siramai, vector search powered everything — recommendations, agents, ranking, conversational search. But CPU-only retrieval was expensive and capped throughput.

So I built a **GPU-accelerated vector search engine** using **FAISS**, **Triton**, and custom GPU kernels. Here's how it worked and why it dramatically improved performance.

## Why GPU Vector Search?

Vector search workloads are:

* **Dense** — operating on high-dimensional embeddings (768-D, 1024-D)
* **Highly parallelizable** — thousands of distance computations can happen simultaneously
* **Memory-bandwidth intensive** — perfect for GPU memory architecture
* **Latency-critical** — users expect sub-100ms response times

CPU FAISS couldn't keep up as we scaled, especially for 768-D embeddings from our transformer models.

## Architecture

```
GPU Memory
  → FAISS IndexFlatIP
  → Triton Distance Kernel
  → Top-K Selection Kernel
  → Results Pipeline
```

## Key Techniques

### 1. Using Half-Precision Embeddings

We quantized embeddings from FP32 → FP16 with negligible accuracy loss:

* **Memory footprint reduced by 50%**
* **2x more vectors fit in GPU memory**
* **Minimal impact on retrieval quality** (< 0.5% recall drop)

```python
# Quantization pipeline
embeddings_fp32 = model.encode(texts)
embeddings_fp16 = embeddings_fp32.astype(np.float16)

# Build FAISS index with FP16
index = faiss.IndexFlatIP(embedding_dim)
gpu_index = faiss.index_cpu_to_gpu(
    faiss.StandardGpuResources(), 
    0, 
    index
)
```

### 2. Triton Kernel for Pairwise Distance Computation

FAISS was fast, but with Triton we got:

* Higher parallelism through custom block sizing
* More control over memory access patterns
* Better memory locality for our specific use case

```python
@triton.jit
def cosine_similarity_kernel(
    query_ptr, db_ptr, output_ptr,
    N, D, BLOCK_SIZE: tl.constexpr
):
    # Custom implementation optimized for our embedding dimensions
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    
    # Load query and database vectors
    # Compute dot products in parallel
    # Write results to output
```

### 3. GPU Pinning for Hot Indices

We pinned high-traffic indices into GPU VRAM:

* Product catalog embeddings (always in GPU memory)
* User preference vectors (cached on GPU)
* Category embeddings (preloaded)

This eliminated CPU → GPU transfer latency for 85% of queries.

## Performance

| Metric       | CPU    | GPU        | Speedup |
| ------------ | ------ | ---------- | ------- |
| 100k vectors | 65 ms  | **3.8 ms** | 17x     |
| 1M vectors   | 510 ms | **27 ms**  | 19x     |
| QPS          | 350    | **5,000+** | 14x     |

## Implementation Challenges

### Memory Management

GPUs have limited memory compared to CPUs. We solved this with:

* **Index sharding** — splitting large indices across multiple GPUs
* **Dynamic loading** — swapping cold indices to CPU when not in use
* **Embedding compression** — using product quantization for very large indices

### Batching Strategy

Single-query latency can be suboptimal on GPUs. We implemented:

* **Request batching** — accumulating 8-16 queries before GPU execution
* **Adaptive batch sizes** — adjusting based on load
* **Priority queuing** — low-latency path for critical queries

## Real-World Impact

After deploying GPU vector search:

* **Search latency dropped from 65ms → 4ms** (p50)
* **System handled 14x more queries** per second
* **Cost per query reduced by 60%** (fewer GPU instances needed than CPU)
* **User engagement increased 12%** due to faster, more responsive search

## Future Work

* **Multi-GPU sharding** — distributing indices across GPU cluster
* **GPU-side reranking** — moving the entire retrieval pipeline to GPU
* **CUDA-accelerated semantic filtering** — pre-filtering with custom kernels
* **Dynamic quantization** — adjusting precision based on query complexity

